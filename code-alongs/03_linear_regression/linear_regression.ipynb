{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c35ea00b",
   "metadata": {},
   "source": [
    "# Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d525bedb",
   "metadata": {},
   "source": [
    "# Advertisement data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8661e4a4",
   "metadata": {},
   "source": [
    "## Short EDA on ads data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ddb7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the data into pandas dataframe\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"advertising.csv\", index_col=0)\n",
    "\n",
    "df.head()\n",
    "\n",
    "# and start doing some EDA\n",
    "\n",
    "print(f\"{df.shape[0]} samples\")\n",
    "print(f\"{df.shape[1]-1} features\") # subtract one as price_unit_area is the label and not    \n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54e856b",
   "metadata": {},
   "source": [
    "-features/independent variable: TV, radio, newspaper\n",
    "\n",
    "-label/target/dependent variable: sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8002b866",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d0ac5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 200 rows/samples\n",
    "# 4 column: 3 features and 1 label\n",
    "\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f90fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "# 1 example plot\n",
    "\n",
    "sns.scatterplot(data = df, x=\"TV\", y=\"Sales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f66efbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#want to plot TV, radio, news vs sales on the 3 axes\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(1,3, figsize=(14,4))\n",
    "sns.scatterplot(data = df, x = \"TV\", y=\"Sales\", ax = axes[0])\n",
    "sns.scatterplot(data = df, x = \"Radio\", y=\"Sales\", ax = axes[1])\n",
    "sns.scatterplot(data = df, x = \"Newspaper\", y=\"Sales\", ax = axes[2])\n",
    "\n",
    "axes[0].set(title = \"TV spending vs sales\")\n",
    "\n",
    "#and so on"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4260c261",
   "metadata": {},
   "source": [
    "same as above, but put into loop to keep it more DRY "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa7b6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,3, figsize=(14,4))\n",
    "\n",
    "features = (\"TV\",\"Radio\", \"Newspaper\")\n",
    "for feature, ax in zip(features, axes.flatten()):\n",
    "    sns.scatterplot(data = df, x = feature, y=\"Sales\", ax = ax)  \n",
    "    ax.set(title=f\"{feature} vs sales\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccd1e41",
   "metadata": {},
   "source": [
    "## Scikit-learn steps\n",
    "\n",
    "See this as a recipe to follow, works for most machine learning algorithms with some modifications\n",
    "\n",
    "steps:\n",
    "\n",
    "0. divide into feature matrix X and label y\n",
    "1. train|test split\n",
    "2. scale dataset ( some algorithms don´t need scaling)\n",
    "3. fit algorithm with training data\n",
    "4. transform training data and test data\n",
    "5. evealuate on test data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c920682",
   "metadata": {},
   "source": [
    "### 0. divide into features X and labe y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5adfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[\"Sales\"]\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671e6009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[[\"TV\", \"Newspaper\"\"Radio\"]]\n",
    "X = df.drop(\"Sales\", axis=1)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f73ae0",
   "metadata": {},
   "source": [
    "common way that i will do the above with tuple unpacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47497431",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y=  df.drop(\"Sales\", axis=1), df[\"Sales\"]\n",
    "\n",
    "X.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2e26a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5b0c51",
   "metadata": {},
   "source": [
    "### 1.train|test split\n",
    "\n",
    "- possible to do manually, but sklearn has this implemented of off the shelf that we can use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef26e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "print(f\"{X_train.shape = }\")\n",
    "print(f\"{X_test.shape = }\")\n",
    "print(f\"{y_train.shape = }\")\n",
    "print(f\"{y_test.shape = }\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bbc21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327fa657",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302453f6",
   "metadata": {},
   "source": [
    "### 2. scale dataset\n",
    "\n",
    "common scaling techniques\n",
    "- min-max also called normalization\n",
    "- feature standardization\n",
    "\n",
    "many algorithms work better or only works when the features as scaled\n",
    "- values \"closer\" to each other\n",
    "\n",
    "for normalization\n",
    "-  $X' = \\frac{X-X_{min}}{X_{max}-X_{min}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f282181b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# instantiate a MinMaxScaler instance\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# important note: fit on X-train and not X_test -> this avoids data leakage\n",
    "scaler.fit(X_train) # use training data to fit the scaler\n",
    "\n",
    "# transforms or scales X_train and X_test\n",
    "scaled_X_train = scaler.transform(X_train)\n",
    "scaled_X_test = scaler.transform(X_test)\n",
    "\n",
    "scaled_X_train.shape, scaled_X_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb08d44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_X_train.min(), scaled_X_train.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539da0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have used parameters X_min abd X_max from X_train to scale X_test\n",
    "# if you get exactly 0 and 1 here then probably you have fit X_test which would leak data\n",
    "scaled_X_test.min(), scaled_X_test.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdf1f41",
   "metadata": {},
   "source": [
    "### 3. Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb24ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model = LinearRegression()\n",
    "# put in training data features and label\n",
    "model.fit(scaled_X_train, y_train)\n",
    "\n",
    "model.intercept_, model.coef_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d0b299",
   "metadata": {},
   "source": [
    "test manual prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "122b15a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.54988164, 0.63709677, 0.52286282])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TV, radio, news but scaled\n",
    "test_sample_feature = scaled_X_test[0]\n",
    "\n",
    "test_sample_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "07f6cbb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction on the test sample\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(16.586730852231778)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = model.coef_\n",
    "w0 = model.intercept_\n",
    "\n",
    "print(\"prediction on the test sample\")\n",
    "w0 + w[0]*test_sample_feature[0] + w[1]*test_sample_feature[1] +  w[2]*test_sample_feature[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de99f009",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb3fb39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15692ce4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0286a3ff",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "538c5a3d",
   "metadata": {},
   "source": [
    "Below is not in class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fef86b",
   "metadata": {},
   "source": [
    "Dependent and independent variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2f746d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df.drop(\"Sales\", axis=\"columns\"), df[\"Sales\"]\n",
    "X.head(2), y.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7077232a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4a51cc",
   "metadata": {},
   "source": [
    "Train|test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7c4c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d5ff4c",
   "metadata": {},
   "source": [
    "Feature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7630bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use normalization here\n",
    "# instantiate an object from the class MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train) # use the training data to fit the scaler\n",
    "\n",
    "# very important that we fit to training data, i.e. use training datas parameters to transform \n",
    "# both training and test data, else if we use test datas parameters to scale test data, we have \n",
    "# leaked data, which might give misleading results \n",
    "scaled_X_train = scaler.transform(X_train)\n",
    "scaled_X_test = scaler.transform(X_test)\n",
    "\n",
    "print(f\"{scaled_X_train.min():.2f} ≤ scaled_X_train ≤ {scaled_X_train.max():.2f}\")\n",
    "print(f\"{scaled_X_test.min():.2f} ≤ scaled_X_test ≤ {scaled_X_test.max():.2f}\") # natural that it isn't [0,1] since we fit to training data \n",
    "\n",
    "# we do not scale our target variable y in this lecture "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e4c3e2",
   "metadata": {},
   "source": [
    "Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2720d07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# this model uses SVD approach for solving normal equation\n",
    "model = LinearRegression()\n",
    "model.fit(scaled_X_train, y_train)\n",
    "print(f\"Parameters: {model.coef_}\")\n",
    "print(f\"Intercept parameter: {model.intercept_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d8409c",
   "metadata": {},
   "source": [
    "Predict on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114f0c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first predict on our test data\n",
    "y_pred = model.predict(scaled_X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889b8acd",
   "metadata": {},
   "source": [
    "Evaluate performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4aaa953",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(f\"MAE: {mae:.2f}, MSE: {mse:.2f}, RMSE: {rmse:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning-sadia-awan-mlops25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
